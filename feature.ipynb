{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b735535",
   "metadata": {},
   "source": [
    "Feature extraction and prediction from a single image using EfficientNetV2 and NLF head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84336c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os, math, sys\n",
    "import torch\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Device:', device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c03080ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA_ROOT = /Users/lemon/Documents/TUD/Thesis/Code/nlf/data\n",
      "PROJDIR   = /Users/lemon/Documents/TUD/Thesis/Code/nlf/data/projects/localizerfields\n"
     ]
    }
   ],
   "source": [
    "# Ensure DATA_ROOT and PROJDIR are set before importing nlf\n",
    "import os\n",
    "# Prefer a local `data/` folder inside the repo if it exists, otherwise use a sensible default\n",
    "repo_data = os.path.abspath('data')\n",
    "if os.path.exists(repo_data) and os.listdir(repo_data):\n",
    "    os.environ.setdefault('DATA_ROOT', repo_data)\n",
    "else:\n",
    "    os.environ.setdefault('DATA_ROOT', os.path.expanduser('~/data/posepile'))\n",
    "# Set PROJDIR relative to DATA_ROOT if not already set\n",
    "os.environ.setdefault('PROJDIR', f\"{os.environ['DATA_ROOT']}/projects/localizerfields\")\n",
    "print('DATA_ROOT =', os.environ['DATA_ROOT'])\n",
    "print('PROJDIR   =', os.environ['PROJDIR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10fa311b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backbone flag: efficientnetv2-s proc-side: 256\n"
     ]
    }
   ],
   "source": [
    "# Initialize FLAGS with EfficientNetV2-S backbone and reasonable defaults\n",
    "from nlf.pt import init as nlf_init\n",
    "nlf_init.initialize([\n",
    "    '--backbone', 'efficientnetv2-s',\n",
    "    '--proc-side', '256',\n",
    "    '--stride-test', '32',\n",
    "    '--stride-train', '32',\n",
    "    '--no-batch-renorm'  # use plain BatchNorm2d to keep things simple here\n",
    "])\n",
    "from simplepyutils import FLAGS\n",
    "print('Backbone flag:', FLAGS.backbone, 'proc-side:', FLAGS.proc_side)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c3139ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backbone ready\n"
     ]
    }
   ],
   "source": [
    "# Build EfficientNetV2 backbone without downloading pretrained weights\n",
    "import torch.nn as nn\n",
    "from nlf.pt.backbones import efficientnet as effnet\n",
    "from nlf.pt.backbones.builder import get_normalizer\n",
    "\n",
    "bn = get_normalizer()  # normalization layer factory configured by FLAGS\n",
    "# weights=None avoids any network download\n",
    "bbone_raw = effnet.efficientnet_v2_s(norm_layer=bn, weights=None)\n",
    "backbone = bbone_raw.features.to(device).eval()\n",
    "\n",
    "# Preprocessing layer used by the repo for EfficientNetV2 (mean=std=0.5)\n",
    "class Preproc(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.register_buffer('mean', torch.tensor([0.5,0.5,0.5]).view(3,1,1))\n",
    "        self.register_buffer('std', torch.tensor([0.5,0.5,0.5]).view(3,1,1))\n",
    "    def forward(self, x):\n",
    "        return (x - self.mean.to(x.dtype)) / self.std.to(x.dtype)\n",
    "\n",
    "preproc = Preproc().to(device)\n",
    "print('Backbone ready')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d19efc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using image: data/dance.jpg\n",
      "Feature tensor: (1, 1280, 8, 8)\n"
     ]
    }
   ],
   "source": [
    "# Load one image from data/ (or fall back to example_image.jpg)\n",
    "img_path = 'data/dance.jpg'\n",
    "if not os.path.exists(img_path):\n",
    "    img_path = 'example_image.jpg'\n",
    "print('Using image:', img_path)\n",
    "img = Image.open(img_path).convert('RGB')\n",
    "transform = T.Compose([T.Resize((FLAGS.proc_side, FLAGS.proc_side)), T.ToTensor()])\n",
    "x = transform(img).unsqueeze(0).to(device)  # [1,3,H,W] in [0,1]\n",
    "\n",
    "with torch.no_grad():\n",
    "    x_in = preproc(x)\n",
    "    feat = backbone(x_in)\n",
    "print('Feature tensor:', tuple(feat.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d725446",
   "metadata": {},
   "source": [
    "Predicting with NLF head from features.\n",
    "If the project assets under PROJDIR are available on this machine (joint info and canonical files),\n",
    "the next cell will also compute 2D/3D predictions from the extracted feature map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "115a98be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded scripted multiperson model from models/nlf_l_multi.torchscript\n",
      "Reusing detector submodule from the scripted model\n",
      "Detector returned 1 boxes\n",
      "Detector-based crop/predict skipped: FileNotFoundError(2, 'No such file or directory')\n",
      "Detector returned 1 boxes\n",
      "Detector-based crop/predict skipped: FileNotFoundError(2, 'No such file or directory')\n"
     ]
    }
   ],
   "source": [
    "# Reuse the pretrained TorchScript detector submodule to get person boxes, then run the NLF head on crops\n",
    "# Steps:\n",
    "# 1) load scripted multiperson model and reuse its `detector` attribute\n",
    "# 2) get boxes for the notebook image\n",
    "# 3) for each box crop the original PIL image, resize to FLAGS.proc_side and compute features with your local backbone\n",
    "# 4) run the NLF head on each crop's feature map and print shapes\n",
    "\n",
    "from pathlib import Path\n",
    "try:\n",
    "    ts_path = Path('models/nlf_l_multi.torchscript')\n",
    "    if not ts_path.exists():\n",
    "        raise FileNotFoundError(f\"TorchScript model not found at {ts_path}\")\n",
    "\n",
    "    ts = torch.jit.load(str(ts_path), map_location=device)\n",
    "    print('Loaded scripted multiperson model from', ts_path)\n",
    "\n",
    "    # Reuse detector submodule\n",
    "    if not hasattr(ts, 'detector'):\n",
    "        raise AttributeError('Loaded scripted model has no `detector` attribute')\n",
    "    detector = ts.detector\n",
    "    print('Reusing detector submodule from the scripted model')\n",
    "\n",
    "    # Read image as uint8 tensor for the detector (the detector expects padded/resized image internally)\n",
    "    import torchvision\n",
    "    img_tensor_u8 = torchvision.io.read_image(img_path).to(device)  # [3,H,W], uint8\n",
    "    img_batch = img_tensor_u8.unsqueeze(0)\n",
    "\n",
    "    # Call the detector (returns list-of-tensors per image)\n",
    "    boxes_list = detector(img_batch, threshold=0.3, nms_iou_threshold=0.7, max_detections=50)\n",
    "    boxes = boxes_list[0] if len(boxes_list) > 0 else torch.empty((0, 5), device=device)\n",
    "    print('Detector returned', boxes.shape[0], 'boxes')\n",
    "\n",
    "    if boxes.shape[0] == 0:\n",
    "        print('No detections; nothing to crop/predict')\n",
    "    else:\n",
    "        # Build weight field + NLFModel once and reuse for all crops\n",
    "        from nlf.pt.models import field as lf_field\n",
    "        from nlf.pt.models.nlf_model import NLFModel\n",
    "        weight_field = lf_field.build_field().to(device).eval()\n",
    "        model = NLFModel(nn.Sequential(preproc, backbone), weight_field, get_normalizer(), 1280).to(device).eval()\n",
    "        canonical = model.canonical_locs().to(device)\n",
    "\n",
    "        results = []\n",
    "        for i in range(boxes.shape[0]):\n",
    "            b = boxes[i].cpu().numpy()  # [cx, cy, w, h, score]\n",
    "            cx, cy, w_box, h_box, score = b.tolist()\n",
    "            left = max(0, int(round(cx - w_box / 2)))\n",
    "            top = max(0, int(round(cy - h_box / 2)))\n",
    "            right = min(img.width, int(round(left + w_box)))\n",
    "            bottom = min(img.height, int(round(top + h_box)))\n",
    "            if right <= left or bottom <= top:\n",
    "                print(f'Invalid box {i}, skipping')\n",
    "                continue\n",
    "\n",
    "            # Crop and resize using PIL to the backbone input size\n",
    "            crop_pil = img.crop((left, top, right, bottom)).resize((FLAGS.proc_side, FLAGS.proc_side))\n",
    "            x_crop = transform(crop_pil).unsqueeze(0).to(device)  # [1,3,H,W] in [0,1]\n",
    "\n",
    "            with torch.no_grad():\n",
    "                x_in = preproc(x_crop)\n",
    "                feat_crop = backbone(x_in)  # feature map for this crop\n",
    "                # run heatmap head on crop feature\n",
    "                coords2d_img, coords3d_rel, uncert = model.heatmap_head.predict_same_canonicals(feat_crop, canonical)\n",
    "                W = H = FLAGS.proc_side\n",
    "                f = 0.5 * W / math.tan(math.radians(55.0 / 2))\n",
    "                K = torch.tensor([[f, 0, W / 2], [0, f, H / 2], [0, 0, 1]], dtype=torch.float32, device=device)[None]\n",
    "                coords3d_abs, uncert_out = model.heatmap_head.reconstruct_absolute(coords2d_img, coords3d_rel, uncert, K)\n",
    "\n",
    "            print(f'Box {i}: score={score:.3f}, crop_pixel_box=[{left},{top},{right},{bottom}], coords3d_abs:', tuple(coords3d_abs.shape))\n",
    "            results.append(dict(box=[left, top, right, bottom], score=float(score), coords3d_abs=coords3d_abs, coords2d_img=coords2d_img))\n",
    "\n",
    "    # expose results to the notebook scope\n",
    "    detector_boxes = boxes\n",
    "    detector_results = results if 'results' in locals() else []\n",
    "\n",
    "except Exception as e:\n",
    "    print('Detector-based crop/predict skipped:', repr(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "85a6d4ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using scripted model at models/nlf_l_multi.torchscript\n",
      "SMPLX fitting skipped: RuntimeError('detector_boxes not found — run the detector cell first')\n"
     ]
    }
   ],
   "source": [
    "# Fit SMPLX using the scripted multiperson model and detector boxes\n",
    "# This reuses the multiperson fitting pipeline (crop_model + fitter) embedded in the TorchScript model.\n",
    "from pathlib import Path\n",
    "try:\n",
    "    ts_path = Path('models/nlf_l_multi.torchscript')\n",
    "    if not ts_path.exists():\n",
    "        raise FileNotFoundError(f\"TorchScript model not found at {ts_path}\")\n",
    "\n",
    "    # load scripted model if not already loaded\n",
    "    if 'ts' not in globals():\n",
    "        ts = torch.jit.load(str(ts_path), map_location=device)\n",
    "    print('Using scripted model at', ts_path)\n",
    "\n",
    "    # Ensure detector_boxes is available\n",
    "    if 'detector_boxes' not in globals():\n",
    "        raise RuntimeError('detector_boxes not found — run the detector cell first')\n",
    "\n",
    "    boxes = detector_boxes\n",
    "    # scripted API expects a list of boxes per image\n",
    "    boxes_list = [boxes]\n",
    "\n",
    "    # Read image tensor (uint8) for the estimator\n",
    "    import torchvision\n",
    "    img_tensor_u8 = torchvision.io.read_image(img_path).to(device)  # [3,H,W]\n",
    "    img_batch = img_tensor_u8.unsqueeze(0)\n",
    "\n",
    "    # Call the estimator that fits parametric model using provided boxes\n",
    "    # There are two exported variants; try the parametric estimator with smplx\n",
    "    try:\n",
    "        result = ts.estimate_parametric_batched(img_batch, boxes_list, model_name='smplx')\n",
    "    except Exception:\n",
    "        # some scripted exports may use estimate_smpl_batched alias\n",
    "        result = ts.estimate_smpl_batched(img_batch, boxes_list, model_name='smplx')\n",
    "\n",
    "    print('Fitting result keys:', list(result.keys()))\n",
    "    # Print common outputs and shapes for the first detected person per image\n",
    "    def shape_of(x):\n",
    "        try:\n",
    "            if isinstance(x, (list, tuple)):\n",
    "                return tuple(x[0].shape) if len(x) > 0 else (0,)\n",
    "            return tuple(x.shape)\n",
    "        except Exception:\n",
    "            return str(type(x))\n",
    "\n",
    "    for k in ['pose','betas','trans','vertices3d','joints3d','vertices2d','joints2d']:\n",
    "        if k in result:\n",
    "            print(k, '->', shape_of(result[k]))\n",
    "\n",
    "    # Expose fitting result in notebook scope\n",
    "    smplx_fit_result = result\n",
    "\n",
    "except Exception as e:\n",
    "    print('SMPLX fitting skipped:', repr(e))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
